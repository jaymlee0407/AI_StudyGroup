# 스터디 그룹 활동 기록

### [2025-00-00] n회차

-   **일시:** 2025-03-26, 17:00 ~ 18:00
-   **장소:** 2층 회의실
-   **참석자:**
    -   김민범, 김소현, 박해진, 이제명
-   **발표 주제:**
-   -   Batch Normalization / Overfitting / Dropout
-   **주요 내용:**

    -   PPT 발표 진행: 박해진
    -   Batch Normalization은 Internal Covariate Shift를 줄일 수 있다.
    -   Layer 가 진행되면서 데이터 분포의 차이가 발생하고 층이 깊어질 수록 더 큰 차이가 발생한다.
    -   mini-batch 단위 각 층마다 BN 사용이 필요하다.
    -   데이터 분포의 평균과 표준편차 값을 사용하여 z, normalization을 수행한다.
    -   Inference 단계에서는 이동평균 값을 이용한 BN을 사용한다 (?)

    -   PPT 발표 진행: 김민범
    -   Overfitting과 Underfitting 은 loss curve를 통해 알 수 있다.
    -   epoch를 돌면서 특정 layer를 끄는 방식의 Dropout을 이용하여 Overfitting 해결가능
    -   early stopping 을 이용하여 과적합 방지 가능
    -

-   **토론 및 피드백:**
-   (김민범) 학습 데이터 분포에서 차이가 있을 때 BN의 역할은? 예를 들어, imbalenced dataset에서 BN이 효과적일 수 있는가?
-   (김소현) overfitting 되었는지 아닌지 어떻게 아는가?
-   -> training loss, validation loss를 확인한다.
-   (김소현) early stopping 은 어떻게 이용할 수 있는가?
-   -> 코드에서 구현하거나, Learning rate scheduler를 사용하는 방법이 있다.

## 스터디 참고 자료 및 링크

-   [이제명]
-   Segmentation Loss 관련 https://www.kaggle.com/code/sungjunghwan/loss-function-of-image-segmentation
-   Pytorch Learning Rate Scheduler 관련 https://sanghyu.tistory.com/113
-   Pytorch Early Stopping 관련 https://ysg2997.tistory.com/7
-
-
-
-

## 다음 회의 예정

-   **주제:** max-pooling, convolution layer..
-   **일시:** 2025-04-02
-   **다음에 공부할 자료**
-   AAA

---

---

_작성일: 2025-03-26_
_최종 작성자: 이제명_
